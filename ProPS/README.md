# ProPS Gemini 3.0 Comparison Project

> Testing whether Gemini 3.0 performs better than previous versions at reinforcement learning using Prompted Policy Search (ProPS)

## ğŸ¯ What is This?

This project implements a **laptop-optimized version** of the ProPS research paper to compare different Gemini models on a reinforcement learning task. You'll discover which Gemini version is best at optimizing policies through in-context learning!

### Quick Facts

- â±ï¸ **Time**: 30-60 minutes for full comparison (Deep Think takes longer but reasons better!)
- ğŸ’» **Requirements**: Python 3.8+, Gemini API key (free tier works!)
- ğŸ® **Task**: Solve MountainCar environment
- ğŸ¤– **Models**: Gemini 3 Deep Think Preview, 2.5 Flash, 2.0 Flash

## ğŸš€ Quick Start (3 Commands)

```bash
# 1. Install dependencies
pip install -r requirements_props.txt

# 2. Run guided experiment
python run_experiment.py

# 3. Analyze results
python analyze_results.py
```

**That's it!** The script will guide you through everything.

> **ğŸ§  Note**: This experiment uses **Gemini 3 Deep Think Preview**, a model designed for complex reasoning. It takes longer per decision (~5-30 seconds vs 1-2 seconds) but should provide better optimization through deeper analysis. See [DEEP_THINK_NOTES.md](DEEP_THINK_NOTES.md) for details!

## ğŸ“– Documentation

| Document | Purpose | Read Time |
|----------|---------|-----------|
| **[INDEX.md](INDEX.md)** | Complete navigation guide | 5 min |
| **[GETTING_STARTED.md](GETTING_STARTED.md)** | Step-by-step setup | 10 min |
| **[PROJECT_SUMMARY.md](PROJECT_SUMMARY.md)** | Full project overview | 15 min |
| **[WORKFLOW.md](WORKFLOW.md)** | Visual workflow diagrams | 10 min |
| **[README_COMPARISON.md](README_COMPARISON.md)** | Technical details | 20 min |

**New here?** Start with [GETTING_STARTED.md](GETTING_STARTED.md)

## ğŸ“ What is ProPS?

**Prompted Policy Search (ProPS)** is a novel RL method that uses Large Language Models as optimizers:

```
Traditional RL:          ProPS:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Neural    â”‚         â”‚     LLM     â”‚
â”‚   Network   â”‚         â”‚  (Gemini)   â”‚
â”‚  + Gradient â”‚         â”‚ + Prompting â”‚
â”‚   Descent   â”‚         â”‚             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
      â†“                       â†“
  Learn policy          Learn policy
  through               through
  backprop              reasoning
```

### Key Innovation

Instead of gradient descent, ProPS:
1. Shows LLM history of (parameters â†’ reward) pairs
2. LLM reasons about which parameters to try next
3. Evaluates suggested parameters
4. Repeats until optimal policy found

**Result**: Competitive with traditional RL, no gradient computation needed!

## ğŸ”¬ The Experiment

### Task: MountainCar

An underpowered car must reach the goal by building momentum:

```
     Goal!
      ğŸ
       /\
      /  \
     /    \___
    /         \
   /           \___
  /                \
 /                  \
ğŸš— â† Start here
```

**Challenge**: Car can't drive straight up. Must learn to swing back and forth!

### What We're Testing

**Research Question**: Does Gemini 3.0 optimize policies better than previous versions?

**Models Compared**:
- Gemini 3 Deep Think Preview â† **New! Deep reasoning model**
- Gemini 2.5 Flash
- Gemini 2.0 Flash

**Metrics**:
- Final reward achieved
- Learning speed
- Consistency
- Execution time

## ğŸ“Š Expected Results

### Good Performance
```
Episode:  1  10  20  30  40  50
Reward:  45  63  75  83  89  91  â† Learning!
```

### What Success Looks Like
- Rewards increase over episodes
- Final reward > 80
- Smooth learning curve
- One model clearly better

## ğŸ› ï¸ Project Structure

```
PrPoS/
â”œâ”€â”€ ğŸ“– Documentation
â”‚   â”œâ”€â”€ INDEX.md                    # Navigation hub
â”‚   â”œâ”€â”€ GETTING_STARTED.md          # Quick start
â”‚   â”œâ”€â”€ PROJECT_SUMMARY.md          # Complete overview
â”‚   â”œâ”€â”€ WORKFLOW.md                 # Visual guide
â”‚   â””â”€â”€ README_COMPARISON.md        # Technical docs
â”‚
â”œâ”€â”€ ğŸ Scripts
â”‚   â”œâ”€â”€ run_experiment.py           # Master script â­
â”‚   â”œâ”€â”€ props_gemini_comparison.py  # Main experiment
â”‚   â”œâ”€â”€ quick_test.py               # Fast validation
â”‚   â”œâ”€â”€ test_setup.py               # Setup check
â”‚   â””â”€â”€ analyze_results.py          # Results analysis
â”‚
â”œâ”€â”€ âš™ï¸ Config
â”‚   â””â”€â”€ requirements_props.txt      # Dependencies
â”‚
â””â”€â”€ ğŸ“Š Results (generated)
    â””â”€â”€ results/
        â”œâ”€â”€ comparison_*.png        # Plots
        â”œâ”€â”€ results_*.json          # Data
        â””â”€â”€ analysis_*.png          # Analysis
```

## ğŸ® Usage Examples

### Example 1: First Time User
```bash
# Read the guide
cat GETTING_STARTED.md

# Run guided experiment
python run_experiment.py
```

### Example 2: Quick Test
```bash
# Just verify everything works (2-5 min)
python quick_test.py
```

### Example 3: Full Comparison
```bash
# Run complete experiment (15-30 min)
python props_gemini_comparison.py

# Analyze results
python analyze_results.py
```

### Example 4: Custom Configuration
```python
# Edit props_gemini_comparison.py
CONFIG = {
    "NUM_EPISODES": 100,  # More episodes
    "GEMINI_MODELS": [
        "gemini-2.0-flash-exp"  # Test only one model
    ]
}
```

## ğŸ“ˆ Sample Output

```
ProPS - Gemini Model Comparison
============================================================

Running experiment with gemini-2.0-flash-exp
============================================================

Running 10 warmup episodes...
  Warmup 1/10: Reward = 42.31
  Warmup 2/10: Reward = 38.92
  ...

Starting training for 50 episodes...
Episode 1/50: Reward = 45.23, Params = [2.1, 3.4]
Episode 2/50: Reward = 52.87, Params = [2.3, 3.8]
...
Episode 50/50: Reward = 91.24, Params = [3.2, 4.1]

Completed in 487.32 seconds
Final Reward: 91.24
Max Reward: 92.15
Avg Reward: 78.43

============================================================
SUMMARY
============================================================

gemini-2.0-flash-exp:
  Final Reward: 91.24
  Max Reward: 92.15
  Avg Reward: 78.43
  Time: 487.32s

ğŸ† Best Model: gemini-2.0-flash-exp
```

## ğŸ”§ Configuration

### Laptop-Optimized (Default)
```python
CONFIG = {
    "NUM_EPISODES": 50,              # 5x faster than paper
    "WARMUP_EPISODES": 10,
    "NUM_EVALUATION_EPISODES": 5,
    "MAX_TRAJ_LENGTH": 500,
}
```
**Time**: 15-30 minutes

### Ultra-Fast (Testing)
```python
CONFIG = {
    "NUM_EPISODES": 10,
    "WARMUP_EPISODES": 3,
    "NUM_EVALUATION_EPISODES": 2,
}
```
**Time**: 2-5 minutes

### High-Quality (Research)
```python
CONFIG = {
    "NUM_EPISODES": 200,
    "WARMUP_EPISODES": 20,
    "NUM_EVALUATION_EPISODES": 20,
}
```
**Time**: 1-2 hours

## ğŸ”‘ Getting API Key

1. Visit [Google AI Studio](https://aistudio.google.com/)
2. Sign in with Google account
3. Click "Get API Key"
4. Create new API key
5. Copy key (starts with "AI...")
6. Paste when script asks

**Free tier includes**: 60 requests/minute, plenty for this experiment!

## ğŸ“Š Analysis Features

The `analyze_results.py` script provides:

- **Learning Curves**: Reward over episodes with confidence intervals
- **Model Comparison**: Bar charts comparing final/max rewards
- **Statistical Summary**: Mean, std, min, max for each model
- **Rankings**: Best model by different metrics
- **LaTeX Export**: Publication-ready tables

## ğŸ› Troubleshooting

| Problem | Solution |
|---------|----------|
| Import errors | `pip install -r requirements_props.txt` |
| API key invalid | Get new key from aistudio.google.com |
| Rate limits | Wait 5-10 min or reduce NUM_EPISODES |
| Low rewards | Normal! Try more episodes |
| Out of memory | Reduce MAX_TRAJ_LENGTH |

**More help**: See [GETTING_STARTED.md](GETTING_STARTED.md) â†’ Troubleshooting

## ğŸ¯ Next Steps

After running the experiment:

1. **Analyze Results**
   - Which model performed best?
   - Was learning consistent?
   - How do results compare to paper?

2. **Run More Experiments**
   - Multiple runs for statistics
   - Different configurations
   - Other environments

3. **Extend the Project**
   - Implement ProPS+ (with environment descriptions)
   - Test other LLMs
   - Compare with traditional RL

4. **Share Findings**
   - Document interesting results
   - Create visualizations
   - Contribute to research

## ğŸ“š Research Context

Based on:
**"Prompted Policy Search (ProPS): Reinforcement Learning through Linguistic and Numerical Reasoning in LLMs"**
- Authors: Zhou et al., 2025
- Paper: See `26193_Prompted_Policy_Search_R.pdf`
- Website: https://props-llm.github.io/

### Key Contributions

- First to use LLMs as RL optimizers via prompting
- Combines numerical optimization with linguistic reasoning
- Achieves competitive performance without gradients
- Enables interpretable policy search

### Your Contribution

By testing Gemini 3.0, you're:
- Extending research to newer models
- Validating ProPS on different setups
- Contributing to understanding of LLM capabilities

## ğŸ¤ Contributing

Found interesting results? Consider:
- Documenting your findings
- Sharing plots and data
- Testing other environments
- Improving the code

## ğŸ“„ License

This project is for educational and research purposes. Original ProPS research by Zhou et al., 2025.

## ğŸ™ Acknowledgments

- Zhou et al. for the ProPS research
- Google for Gemini API
- OpenAI Gym/Gymnasium for environments

## ğŸ“ Quick Links

- **Start Here**: [GETTING_STARTED.md](GETTING_STARTED.md)
- **Full Guide**: [PROJECT_SUMMARY.md](PROJECT_SUMMARY.md)
- **Visual Guide**: [WORKFLOW.md](WORKFLOW.md)
- **Navigation**: [INDEX.md](INDEX.md)
- **API Key**: https://aistudio.google.com/
- **ProPS Website**: https://props-llm.github.io/

---

## ğŸš€ Ready to Start?

```bash
# Windows users
run.bat

# Everyone else
python run_experiment.py
```

**Good luck with your experiment! ğŸ‰**

If you find that Gemini 3.0 performs better, that's a valuable contribution to the research community!
